markdown# Handwritten Digit Recognition - Neural Network From Scratch



A fully connected neural network built \*\*completely from scratch\*\* using only pure Python to recognize handwritten digits from the MNIST dataset. No NumPy, TensorFlow, or PyTorch used for the neural network implementation!



\## ğŸ¯ Project Overview



This project implements a neural network from absolute scratch to classify handwritten digits (0-9) from the MNIST dataset, achieving \*\*95-97% accuracy\*\* without using any deep learning frameworks.



\## âœ¨ Key Features



\- âœ… \*\*Pure Python Implementation\*\* - Neural network built from ground up

\- âœ… \*\*No NumPy\*\* - Custom matrix operations library

\- âœ… \*\*No TensorFlow/PyTorch\*\* - Complete backpropagation implementation

\- âœ… \*\*Custom Matrix Class\*\* - All linear algebra operations coded manually

\- âœ… \*\*Mini-batch Gradient Descent\*\* - Efficient training optimization

\- âœ… \*\*He Weight Initialization\*\* - Stable training convergence

\- âœ… \*\*ReLU + Softmax Activations\*\* - Modern activation functions

\- âœ… \*\*95-97% Test Accuracy\*\* - Competitive performance



\## ğŸ—ï¸ ArchitectureInput Layer (784 neurons - 28x28 pixels)

â†“

Hidden Layer (128 neurons - ReLU activation)

â†“

Output Layer (10 neurons - Softmax activation)



\## ğŸ“ Project StructureDigitRecognition/

â”œâ”€â”€ my\_math.py                    # Custom matrix operations library

â”œâ”€â”€ data\_loader.py                # MNIST dataset loader

â”œâ”€â”€ neural\_network\_scratch.py     # Neural network implementation

â”œâ”€â”€ train.py                      # Main training script

â”œâ”€â”€ test\_single.py               # Single prediction testing

â”œâ”€â”€ find\_data.py                 # Data location finder

â”œâ”€â”€ README.md                    # Project documentation

â””â”€â”€ requirements.txt             # Python dependencies



\## ğŸš€ Getting Started



\### Prerequisites



\- Python 3.7+

\- matplotlib (only for visualization)



\### Installation



1\. Clone the repository:

```bashgit clone https://github.com/kaushalrog/digitrecognition.git

cd digitrecognition



2\. Create virtual environment:

```bashpython -m venv venv

.\\venv\\Scripts\\Activate.ps1  # On Windows

source venv/bin/activate    # On Linux/Mac



3\. Install dependencies:

```bashpip install -r requirements.txt



4\. Download MNIST dataset:

&nbsp;  - Visit: http://yann.lecun.com/exdb/mnist/

&nbsp;  - Download all 4 files (.gz format)

&nbsp;  - Place in `data/MNIST/` folder



\### Running the Project

```bashTrain the neural network

python train.pyTest single predictions

python test\_single.pyFind MNIST data location

python find\_data.py



\## ğŸ“Š Results



\- \*\*Training Accuracy\*\*: 96-98%

\- \*\*Test Accuracy\*\*: 95-97%

\- \*\*Training Time\*\*: ~20 epochs (10-15 minutes on CPU)

\- \*\*Model Size\*\*: Lightweight (~100KB parameters)



\## ğŸ§® Implementation Details



\### Custom Components Built From Scratch:



1\. \*\*Matrix Class\*\*: Complete matrix operations

&nbsp;  - Matrix multiplication (dot product)

&nbsp;  - Transpose

&nbsp;  - Element-wise operations

&nbsp;  - Broadcasting support



2\. \*\*Activation Functions\*\*:

&nbsp;  - ReLU (Rectified Linear Unit)

&nbsp;  - Softmax (for output probabilities)

&nbsp;  - Derivatives for backpropagation



3\. \*\*Forward Propagation\*\*:

&nbsp;  - Linear transformations

&nbsp;  - Activation functions

&nbsp;  - Layer-wise computation



4\. \*\*Backpropagation\*\*:

&nbsp;  - Gradient computation

&nbsp;  - Chain rule implementation

&nbsp;  - Weight update mechanism



5\. \*\*Training Algorithm\*\*:

&nbsp;  - Mini-batch gradient descent

&nbsp;  - Cross-entropy loss

&nbsp;  - Data shuffling

&nbsp;  - Epoch-based training



\## ğŸ“š Learning Outcomes



This project demonstrates deep understanding of:

\- Neural network fundamentals

\- Matrix-based computation

\- Gradient descent optimization

\- Backpropagation algorithm

\- Activation functions

\- Multi-class classification

\- Mini-batch training



\## ğŸ› ï¸ Technologies Used



\- \*\*Python\*\* - Core programming language

\- \*\*Pure Python\*\* - Neural network implementation

\- \*\*matplotlib\*\* - Visualization only



\## ğŸ“ˆ Training OutputEpoch  1/20 | Loss: 0.4234 | Train Acc: 88.45% | Test Acc: 88.92%

Epoch  2/20 | Loss: 0.2156 | Train Acc: 93.21% | Test Acc: 93.54%

...

Epoch 20/20 | Loss: 0.0891 | Train Acc: 97.34% | Test Acc: 96.12%



\## ğŸ“ Mathematical Foundations



\### Forward Pass:Z1 = XÂ·W1 + b1

A1 = ReLU(Z1)

Z2 = A1Â·W2 + b2

A2 = Softmax(Z2)



\### Loss Function:Loss = -1/m Ã— Î£(Y\_true Ã— log(Y\_pred))



\### Backpropagation:dZ2 = A2 - Y\_true

dW2 = A1áµ€Â·dZ2 / m

dZ1 = (dZ2Â·W2áµ€) âŠ™ ReLU'(Z1)

dW1 = Xáµ€Â·dZ1 / m



\## ğŸ¤ Contributing



Contributions are welcome! Feel free to:

\- Report bugs

\- Suggest features

\- Submit pull requests



\## ğŸ“ License



This project is open source and available under the MIT License.



\## ğŸ‘¤ Author



\*\*Kaushal\*\*

\- GitHub: \[@kaushalrog](https://github.com/kaushalrog)



\## ğŸ™ Acknowledgments



\- MNIST Dataset by Yann LeCun

\- Neural Networks and Deep Learning concepts

\- Pure Python implementation challenge



\## ğŸ“§ Contact



For questions or feedback, please open an issue on GitHub.



---



â­ If you found this project helpful, please give it a star!

